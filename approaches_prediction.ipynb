{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c897e3525e2cb08e",
   "metadata": {},
   "source": [
    "Connecting to the Database and clean the data \n",
    "\n",
    "Data structure explanation: \n",
    "\n",
    "junctionID - a unique ID number assigned to whole junction\n",
    "\n",
    "approachID – a unique ID number assigned to this specific approach line\n",
    "\n",
    "travelTimeSec - information on how long (in seconds) it currently takes for the vehicles to travel through the approach\n",
    "\n",
    "freeFlowTravelTime - information on how long (in seconds) it takes for the vehicles to travel through the approach during free-flow conditions( when there is no traffic jams, etc)\n",
    "\n",
    "delaySec - the current delay (in seconds) \n",
    "\n",
    "usualDelaySec - this is the usual delay that is expected at this time of day\n",
    "\n",
    "Stops - the average number of stops per vehicle on the approach\n",
    "\n",
    "queueLengthMeters - information on how long (in meters) the current line of vehicles waiting on the junction approach is\n",
    "\n",
    "volumePerHour - an estimated value of how many vehicles we observe traveling through the approach per hour\n",
    "\n",
    "isClosed – information if the approach was closed for traffic \n",
    "\n",
    "stopsHistogram - a container that stores histogram data specific to stops ( example: 0,3 is displaying the data like this: \"numberOfStops\": 0, \"numberOfVehicles\": 3) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d18bde48773f8e5",
   "metadata": {},
   "source": [
    "Fistly, I connected to the Database through SQLAlchemy engine and get the 'approaches' and 'traffic-main' tables. Then, I cleaned 'approaches' table. After cleaning I merged 2 tables data (especially, all the columns from 'approaches' table and only 'direction' column from 'traffic-main' table) using JOIN function. Finally, I store all the data in new table called 'updated_approaches'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e6453d7a4b91d1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T08:57:11.268368Z",
     "start_time": "2024-10-31T08:57:10.429178Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Database connection parameters\n",
    "db_params = {\n",
    "    'dbname': 'traffic-system',\n",
    "    'user': 'postgres',\n",
    "    'password': '1234',\n",
    "    'host': 'localhost',\n",
    "    'port': '5432'\n",
    "}\n",
    "\n",
    "# Create a connection string\n",
    "connection_string = f\"postgresql://{db_params['user']}:{db_params['password']}@\" \\\n",
    "                    f\"{db_params['host']}:{db_params['port']}/{db_params['dbname']}\"\n",
    "\n",
    "# Create a SQLAlchemy engine\n",
    "engine = create_engine(connection_string)\n",
    "\n",
    "# Step 1: Load data from 'approaches' table into a DataFrame\n",
    "query_approaches = \"SELECT * FROM approaches;\"\n",
    "df = pd.read_sql(query_approaches, engine)\n",
    "\n",
    "# Step 2: Data Cleaning\n",
    "# Drop rows with any missing values\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Drop rows where 'stopshistogram' is null\n",
    "df.dropna(subset=['stopshistogram'], inplace=True)\n",
    "\n",
    "# Drop the 'junctionid' column if it exists\n",
    "df.drop(columns=['junctionid'], inplace=True, errors='ignore')\n",
    "\n",
    "# Replace 'approaches' table with cleaned data\n",
    "df.to_sql('approaches', engine, if_exists='replace', index=False)\n",
    "\n",
    "# Step 3: Join cleaned data with 'traffic_main' on 'approachid' and 'id'\n",
    "query_join = \"\"\"\n",
    "SELECT a.*, t.direction\n",
    "FROM approaches a\n",
    "JOIN traffic_main t ON a.approachid = t.id;\n",
    "\"\"\"\n",
    "df_joined = pd.read_sql(query_join, engine)\n",
    "\n",
    "# Step 4: Save the joined DataFrame to a new table 'updated_approaches'\n",
    "df_joined.to_sql('updated_approaches', engine, if_exists='replace', index=False)\n",
    "\n",
    "# Display the first few rows and the number of rows in the joined DataFrame\n",
    "print(df_joined.head())\n",
    "print(f\"Number of rows in the joined DataFrame: {df_joined.shape[0]}\")\n",
    "\n",
    "engine.dispose()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90cbba4e3cbb9a0",
   "metadata": {},
   "source": [
    "Now, connect to the database again and merge the data from 'weather.csv' and the data from 'updated_approaches' table in 'traffic-system' DB. \n",
    "\n",
    "To do that I created a new columns in  'updated_approaches' that will store the time data (especially, the time data excluding minutes and seconds). Then, as above, I JOINed 2 tables and stored that table I got from the JOIN to the new table called 'merged_traffic_weather'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "66e7c737d5b70f67",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T08:57:11.573966Z",
     "start_time": "2024-10-31T08:57:11.363664Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'psycopg2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 18\u001b[0m\n\u001b[1;32m     14\u001b[0m connection_string \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpostgresql://\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdb_params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdb_params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpassword\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m@\u001b[39m\u001b[38;5;124m\"\u001b[39m \\\n\u001b[1;32m     15\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdb_params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhost\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdb_params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mport\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdb_params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdbname\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Create a SQLAlchemy engine\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m engine \u001b[38;5;241m=\u001b[39m create_engine(connection_string)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Step 1: Load data from 'updated_approaches' table\u001b[39;00m\n\u001b[1;32m     21\u001b[0m query_updated_approaches \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSELECT * FROM updated_approaches;\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m<string>:2\u001b[0m, in \u001b[0;36mcreate_engine\u001b[0;34m(url, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sqlalchemy/util/deprecations.py:281\u001b[0m, in \u001b[0;36mdeprecated_params.<locals>.decorate.<locals>.warned\u001b[0;34m(fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m kwargs:\n\u001b[1;32m    275\u001b[0m         _warn_with_version(\n\u001b[1;32m    276\u001b[0m             messages[m],\n\u001b[1;32m    277\u001b[0m             versions[m],\n\u001b[1;32m    278\u001b[0m             version_warnings[m],\n\u001b[1;32m    279\u001b[0m             stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[1;32m    280\u001b[0m         )\n\u001b[0;32m--> 281\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sqlalchemy/engine/create.py:599\u001b[0m, in \u001b[0;36mcreate_engine\u001b[0;34m(url, **kwargs)\u001b[0m\n\u001b[1;32m    597\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m kwargs:\n\u001b[1;32m    598\u001b[0m             dbapi_args[k] \u001b[38;5;241m=\u001b[39m pop_kwarg(k)\n\u001b[0;32m--> 599\u001b[0m     dbapi \u001b[38;5;241m=\u001b[39m dbapi_meth(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdbapi_args)\n\u001b[1;32m    601\u001b[0m dialect_args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdbapi\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m dbapi\n\u001b[1;32m    603\u001b[0m dialect_args\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompiler_linting\u001b[39m\u001b[38;5;124m\"\u001b[39m, compiler\u001b[38;5;241m.\u001b[39mNO_LINTING)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sqlalchemy/dialects/postgresql/psycopg2.py:690\u001b[0m, in \u001b[0;36mPGDialect_psycopg2.import_dbapi\u001b[0;34m(cls)\u001b[0m\n\u001b[1;32m    688\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    689\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mimport_dbapi\u001b[39m(\u001b[38;5;28mcls\u001b[39m):\n\u001b[0;32m--> 690\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpsycopg2\u001b[39;00m\n\u001b[1;32m    692\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m psycopg2\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'psycopg2'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Database connection parameters\n",
    "db_params = {\n",
    "    'dbname': 'traffic-system',\n",
    "    'user': 'postgres',\n",
    "    'password': '1234',\n",
    "    'host': 'localhost',\n",
    "    'port': '5432'\n",
    "}\n",
    "\n",
    "# Create a connection string\n",
    "connection_string = f\"postgresql://{db_params['user']}:{db_params['password']}@\" \\\n",
    "                    f\"{db_params['host']}:{db_params['port']}/{db_params['dbname']}\"\n",
    "\n",
    "# Create a SQLAlchemy engine\n",
    "engine = create_engine(connection_string)\n",
    "\n",
    "# Step 1: Load data from 'updated_approaches' table\n",
    "query_updated_approaches = \"SELECT * FROM updated_approaches;\"\n",
    "df_updated_approaches = pd.read_sql(query_updated_approaches, engine)\n",
    "\n",
    "# Step 2: Create a new column 'rounded_time' that floors 'time' to the nearest hour\n",
    "df_updated_approaches['rounded_time'] = pd.to_datetime(df_updated_approaches['time']).dt.floor('H')\n",
    "\n",
    "# Step 3: Load weather data from CSV and convert 'time' to datetime format in UTC\n",
    "weather_file_path = '/Users/asylkaziahmetova/Desktop/bit/traffic-management/data/weather.csv'\n",
    "df_weather = pd.read_csv(weather_file_path)\n",
    "df_weather['time'] = pd.to_datetime(df_weather['time'], format='%Y-%m-%dT%H:%M').dt.tz_localize('UTC')\n",
    "\n",
    "# Step 4: Merge the DataFrames on 'rounded_time' and 'time' from weather data\n",
    "df_merged = pd.merge(df_updated_approaches, df_weather, left_on='rounded_time', right_on='time', how='inner')\n",
    "\n",
    "# Step 5: Save the merged DataFrame to a new table in the database\n",
    "df_merged.to_sql('merged_traffic_weather', engine, if_exists='replace', index=False)\n",
    "\n",
    "# Display the first few rows and the number of rows in the merged DataFrame\n",
    "print(df_merged.head())\n",
    "print(f\"Number of rows in the merged DataFrame: {df_merged.shape[0]}\")\n",
    "\n",
    "# Close the engine connection\n",
    "engine.dispose()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940ae71213808db3",
   "metadata": {},
   "source": [
    "Correlation analysis.\n",
    "\n",
    "To find the pairwise relationships between the selected features a correlation matrix was created. Then heatmap visualization of this matrix was used for accessing. Overall, I accessed the correlation of the Traffic Volume and Weather data correlations.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e47e13f35f48de0d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T08:57:11.705096Z",
     "start_time": "2024-10-31T08:57:11.701271Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_merged' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Select only the relevant columns for correlation analysis\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m df_corr \u001b[38;5;241m=\u001b[39m df_merged[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdelaysec\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtemperature_2m (°C)\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelative_humidity_2m (\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m      3\u001b[0m                      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mapparent_temperature (°C)\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrain (mm)\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwind_speed_10m (km/h)\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m      4\u001b[0m                      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstops\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime_x\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvolumeperhour\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mqueuelengthmeters\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[1;32m      5\u001b[0m correlation_matrix \u001b[38;5;241m=\u001b[39m df_corr\u001b[38;5;241m.\u001b[39mcorr()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_merged' is not defined"
     ]
    }
   ],
   "source": [
    "# Select only the relevant columns for correlation analysis\n",
    "df_corr = df_merged[['delaysec', 'temperature_2m (°C)', 'relative_humidity_2m (%)', \n",
    "                     'apparent_temperature (°C)', 'rain (mm)', 'wind_speed_10m (km/h)', \n",
    "                     'stops','time_x','volumeperhour', 'queuelengthmeters']]\n",
    "correlation_matrix = df_corr.corr()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dfd8dc2958a1719f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T08:57:12.029602Z",
     "start_time": "2024-10-31T08:57:11.714188Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_corr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Calculate the correlation matrix\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m correlation_matrix \u001b[38;5;241m=\u001b[39m df_corr\u001b[38;5;241m.\u001b[39mcorr()\n\u001b[1;32m      7\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n\u001b[1;32m      8\u001b[0m sns\u001b[38;5;241m.\u001b[39mheatmap(correlation_matrix, annot\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, cmap\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcoolwarm\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[1;32m      9\u001b[0m             vmin\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, vmax\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, fmt\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;124m\"\u001b[39m, linewidths\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_corr' is not defined"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate the correlation matrix\n",
    "correlation_matrix = df_corr.corr()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\", \n",
    "            vmin=-1, vmax=1, fmt=\".2f\", linewidths=0.5)\n",
    "\n",
    "plt.title(\"Correlation Matrix of Traffic Delay and Weather Features\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7fc3d436ed4b387",
   "metadata": {},
   "source": [
    "Split the data into train and test (80 and 20%) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcdeda8c3443f10c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T08:57:12.406553Z",
     "start_time": "2024-10-31T08:57:12.056037Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df_merged[['temperature_2m (°C)', 'relative_humidity_2m (%)', \n",
    "                'apparent_temperature (°C)', 'rain (mm)', \n",
    "                'wind_speed_10m (km/h)', 'stops', \n",
    "                'queuelengthmeters']]\n",
    "y = df_merged['volumeperhour']  \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                test_size=0.2, random_state=42)\n",
    "print(X_train.shape) \n",
    "print(y_train.shape) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af0cb8ce0118ce0",
   "metadata": {},
   "source": [
    "Train the RandomForestRegressor model and fit the train data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e093d1d60b4c7685",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T08:57:13.070871Z",
     "start_time": "2024-10-31T08:57:12.490279Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "model = RandomForestRegressor()\n",
    "model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35dfb75b0e8ec141",
   "metadata": {},
   "source": [
    "Use the model to predict on the test data and make evaluations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c083877af1fca38b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T08:57:13.138769Z",
     "start_time": "2024-10-31T08:57:13.116769Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\" R2: {r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70338f69f957b41",
   "metadata": {},
   "source": [
    "Smart traffic light timing adjustment using predictions made by the RandomForestRegressor model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9278c011454096b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T08:57:13.231113Z",
     "start_time": "2024-10-31T08:57:13.224214Z"
    }
   },
   "outputs": [],
   "source": [
    "def adjust_traffic_lights(predictions, initial_timings={'Green': 60, 'Yellow': 5, 'Red': 30}):\n",
    "    traffic_light_timings = []\n",
    "\n",
    "    for volume in predictions:\n",
    "        green_time = initial_timings['Green']\n",
    "        yellow_time = initial_timings['Yellow']\n",
    "        red_time = initial_timings['Red']\n",
    "\n",
    "        if volume < 100:\n",
    "            green_time = 60\n",
    "        elif 100 <= volume < 500:\n",
    "            green_time = 50\n",
    "        elif 500 <= volume < 1000:\n",
    "            green_time = 40\n",
    "        elif 1000 <= volume < 3000:\n",
    "            green_time = 30\n",
    "        else:  # volume >= 3000\n",
    "            green_time = 20\n",
    "\n",
    "        traffic_light_timings.append(f\"Green: {green_time} seconds, Yellow: {yellow_time} seconds, Red: {red_time} seconds\")\n",
    "\n",
    "    return traffic_light_timings\n",
    "\n",
    "predictions = y_pred[:100] \n",
    "traffic_light_adjustments = adjust_traffic_lights(predictions)\n",
    "\n",
    "for i, adjustment in enumerate(traffic_light_adjustments):\n",
    "    print(f\"Prediction {i + 1}: Volume = {predictions[i]}, Adjustments = {adjustment}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957fc3329e28fb76",
   "metadata": {},
   "source": [
    "Additional model to update the traffic light based on each line (on a crossroad) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05be8a9d595edb5",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-10-31T09:00:54.685380Z"
    },
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "import time\n",
    "\n",
    "# Database connection parameters\n",
    "db_params = {\n",
    "    'dbname': 'traffic-system',\n",
    "    'user': 'postgres',\n",
    "    'password': '1234',\n",
    "    'host': 'localhost',\n",
    "    'port': '5432'\n",
    "}\n",
    "\n",
    "# SQLAlchemy connection string\n",
    "connection_string = f\"postgresql://{db_params['user']}:{db_params['password']}@\" \\\n",
    "                    f\"{db_params['host']}:{db_params['port']}/{db_params['dbname']}\"\n",
    "engine = create_engine(connection_string)\n",
    "\n",
    "# Initial timing (in seconds) for each direction\n",
    "initial_green_time = {\n",
    "    'NORTH': 30,\n",
    "    'SOUTH': 30,\n",
    "    'EAST': 30,\n",
    "    'WEST': 30\n",
    "}\n",
    "\n",
    "# Threshold parameters\n",
    "THRESHOLD_TRAVEL_RATIO = 1.2  # 20% over free-flow time\n",
    "THRESHOLD_DELAY_RATIO = 1.2  # 20% over usual delay\n",
    "GREEN_TIME_INCREMENT = 10  # seconds\n",
    "\n",
    "# Step 1: Initialize Traffic Light Timing\n",
    "traffic_light_timings = initial_green_time.copy()\n",
    "\n",
    "# Function to detect heavy traffic\n",
    "def is_heavy_traffic(row):\n",
    "    traffic_condition = (\n",
    "        row['traveltimesec'] > row['freeflowtraveltimesec'] * THRESHOLD_TRAVEL_RATIO or\n",
    "        row['delaysec'] > row['usualdelaysec'] * THRESHOLD_DELAY_RATIO\n",
    "    )\n",
    "    return traffic_condition\n",
    "\n",
    "# Step 2 & 3: Detect and Adjust Traffic Light Timing Based on Congestion\n",
    "def adjust_traffic_lights():\n",
    "    # Fetch the data from the database\n",
    "    query = \"SELECT * FROM updated_approaches;\"\n",
    "    df = pd.read_sql(query, engine)\n",
    "    \n",
    "    # Group data by timestamp\n",
    "    grouped_by_timestamp = df.groupby('time')\n",
    "    \n",
    "    for timestamp, group in grouped_by_timestamp:\n",
    "        congested_directions = {}\n",
    "\n",
    "        # Check each row in the timestamp group for heavy traffic\n",
    "        for _, row in group.iterrows():\n",
    "            # Avoid division by zero by setting a minimum value of 1 for these columns\n",
    "            freeflow_time = row['freeflowtraveltimesec'] if row['freeflowtraveltimesec'] != 0 else 1\n",
    "            usual_delay = row['usualdelaysec'] if row['usualdelaysec'] != 0 else 1\n",
    "            \n",
    "            if is_heavy_traffic(row):\n",
    "                direction = row['direction']\n",
    "                congestion_level = (\n",
    "                    (row['traveltimesec'] / freeflow_time) +\n",
    "                    (row['delaysec'] / usual_delay)\n",
    "                )\n",
    "                congested_directions[direction] = congestion_level\n",
    "\n",
    "        # Step 4: Adjust the green light for the most congested direction\n",
    "        if congested_directions:\n",
    "            most_congested_direction = max(congested_directions, key=congested_directions.get)\n",
    "            traffic_light_timings[most_congested_direction] += GREEN_TIME_INCREMENT\n",
    "            print(f\"Adjusted green light for {most_congested_direction}: {traffic_light_timings[most_congested_direction]} seconds\")\n",
    "\n",
    "    return traffic_light_timings\n",
    "\n",
    "# Simulation loop for real-time adjustments\n",
    "def run_traffic_light_system():\n",
    "    while True:\n",
    "        current_timings = adjust_traffic_lights()\n",
    "        print(\"Current traffic light timings:\", current_timings)\n",
    "        \n",
    "        # Sleep for the cycle duration before the next adjustment\n",
    "        time.sleep(10)\n",
    "\n",
    "# Start the traffic light system\n",
    "run_traffic_light_system()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
